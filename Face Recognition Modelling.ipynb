{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:#0b486b\">Facial Recognition Model</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "from sklearn import datasets\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display face images from the dataset\n",
    "%matplotlib inline  \n",
    "\n",
    "def display_faces(images, label, num2display):\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "    for i in range(num2display):\n",
    "        p = fig.add_subplot(20,20,i+1,xticks=[],yticks=[])\n",
    "        p.imshow(images[i], cmap=plt.cm.bone)\n",
    "        \n",
    "        p.text(0, 14, str(label[i]))\n",
    "        p.text(0, 60, str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You may need to install the *Pillow* package first by running the following command in the conda prompt."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install --channel conda-forge pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download the dataset to your computer\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = datasets.fetch_lfw_people(min_faces_per_person=80, resize=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store features in variable X and the label in variable y\n",
    "\n",
    "X, y = faces.data, faces.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _labeled_faces_in_the_wild_dataset:\n",
      "\n",
      "The Labeled Faces in the Wild face recognition dataset\n",
      "------------------------------------------------------\n",
      "\n",
      "This dataset is a collection of JPEG pictures of famous people collected\n",
      "over the internet, all details are available on the official website:\n",
      "\n",
      "    http://vis-www.cs.umass.edu/lfw/\n",
      "\n",
      "Each picture is centered on a single face. The typical task is called\n",
      "Face Verification: given a pair of two pictures, a binary classifier\n",
      "must predict whether the two images are from the same person.\n",
      "\n",
      "An alternative task, Face Recognition or Face Identification is:\n",
      "given the picture of the face of an unknown person, identify the name\n",
      "of the person by referring to a gallery of previously seen pictures of\n",
      "identified persons.\n",
      "\n",
      "Both Face Verification and Face Recognition are tasks that are typically\n",
      "performed on the output of a model trained to perform Face Detection. The\n",
      "most popular model for Face Detection is called Viola-Jones and is\n",
      "implemented in the OpenCV library. The LFW faces were extracted by this\n",
      "face detector from various online websites.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   =======================\n",
      "    Classes                                5749\n",
      "    Samples total                         13233\n",
      "    Dimensionality                         5828\n",
      "    Features            real, between 0 and 255\n",
      "    =================   =======================\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "``scikit-learn`` provides two loaders that will automatically download,\n",
      "cache, parse the metadata files, decode the jpeg and convert the\n",
      "interesting slices into memmapped numpy arrays. This dataset size is more\n",
      "than 200 MB. The first load typically takes more than a couple of minutes\n",
      "to fully decode the relevant part of the JPEG files into numpy arrays. If\n",
      "the dataset has  been loaded once, the following times the loading times\n",
      "less than 200ms by using a memmapped version memoized on the disk in the\n",
      "``~/scikit_learn_data/lfw_home/`` folder using ``joblib``.\n",
      "\n",
      "The first loader is used for the Face Identification task: a multi-class\n",
      "classification task (hence supervised learning)::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_lfw_people\n",
      "  >>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
      "\n",
      "  >>> for name in lfw_people.target_names:\n",
      "  ...     print(name)\n",
      "  ...\n",
      "  Ariel Sharon\n",
      "  Colin Powell\n",
      "  Donald Rumsfeld\n",
      "  George W Bush\n",
      "  Gerhard Schroeder\n",
      "  Hugo Chavez\n",
      "  Tony Blair\n",
      "\n",
      "The default slice is a rectangular shape around the face, removing\n",
      "most of the background::\n",
      "\n",
      "  >>> lfw_people.data.dtype\n",
      "  dtype('float32')\n",
      "\n",
      "  >>> lfw_people.data.shape\n",
      "  (1288, 1850)\n",
      "\n",
      "  >>> lfw_people.images.shape\n",
      "  (1288, 50, 37)\n",
      "\n",
      "Each of the ``1140`` faces is assigned to a single person id in the ``target``\n",
      "array::\n",
      "\n",
      "  >>> lfw_people.target.shape\n",
      "  (1288,)\n",
      "\n",
      "  >>> list(lfw_people.target[:10])\n",
      "  [5, 6, 3, 1, 0, 1, 3, 4, 3, 0]\n",
      "\n",
      "The second loader is typically used for the face verification task: each sample\n",
      "is a pair of two picture belonging or not to the same person::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_lfw_pairs\n",
      "  >>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n",
      "\n",
      "  >>> list(lfw_pairs_train.target_names)\n",
      "  ['Different persons', 'Same person']\n",
      "\n",
      "  >>> lfw_pairs_train.pairs.shape\n",
      "  (2200, 2, 62, 47)\n",
      "\n",
      "  >>> lfw_pairs_train.data.shape\n",
      "  (2200, 5828)\n",
      "\n",
      "  >>> lfw_pairs_train.target.shape\n",
      "  (2200,)\n",
      "\n",
      "Both for the :func:`sklearn.datasets.fetch_lfw_people` and\n",
      ":func:`sklearn.datasets.fetch_lfw_pairs` function it is\n",
      "possible to get an additional dimension with the RGB color channels by\n",
      "passing ``color=True``, in that case the shape will be\n",
      "``(2200, 2, 62, 47, 3)``.\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_lfw_pairs` datasets is subdivided into\n",
      "3 subsets: the development ``train`` set, the development ``test`` set and\n",
      "an evaluation ``10_folds`` set meant to compute performance metrics using a\n",
      "10-folds cross validation scheme.\n",
      "\n",
      ".. topic:: References:\n",
      "\n",
      " * `Labeled Faces in the Wild: A Database for Studying Face Recognition\n",
      "   in Unconstrained Environments.\n",
      "   <http://vis-www.cs.umass.edu/lfw/lfw.pdf>`_\n",
      "   Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\n",
      "   University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.\n",
      "\n",
      "\n",
      "Examples\n",
      "~~~~~~~~\n",
      "\n",
      ":ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(faces.DESCR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1140, 1850)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of data points\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0 1 2 3 4]\n",
      "['Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder'\n",
      " 'Tony Blair']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# printing out the number of classes, and corresponding name of each class, each class refers to a person in the dataset\n",
    "class_ids=np.unique(y)\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "print(n_classes)\n",
    "print(class_ids)\n",
    "\n",
    "print(faces.target_names)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 unique classes. 0 corresponds with Colin Powell and 4 with Tony Blair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Facial regonition is a regression based classification problem in this example. Three machine learning algorithms that are suitable are Linear Regression, Linear Discrimnant Analysis, and Support Vector Machine\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into a training and testing set, 70% train, 30% test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:\t798\n",
      "Number of testing samples:\t342\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples:\\t{len(X_train)}\")\n",
    "print(f\"Number of testing samples:\\t{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=80))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=80))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=80)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=80))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Scikit-Learn to conduct a PCA for dimensionality reduction\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA()\n",
    "\n",
    "pipe_face_recognition = Pipeline(steps=[('pca', pca)])\n",
    "pipe_face_recognition.set_params(pca__n_components=80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Scikit-Learn logistc regression for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression(penalty='l1', dual=False,\\\n",
    "              tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,\\\n",
    "              class_weight=None, random_state=None, solver='liblinear', \\\n",
    "            max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pca', PCA(n_components=80)),\n",
      "                ('logistic',\n",
      "                 LogisticRegression(multi_class='ovr', n_jobs=1, penalty='l1',\n",
      "                                    solver='liblinear'))])\n"
     ]
    }
   ],
   "source": [
    "pipe_face_recognition.steps.append(('logistic', logistic))\n",
    "print(pipe_face_recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=80)),\n",
       "                (&#x27;logistic&#x27;,\n",
       "                 LogisticRegression(multi_class=&#x27;ovr&#x27;, n_jobs=1, penalty=&#x27;l1&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=80)),\n",
       "                (&#x27;logistic&#x27;,\n",
       "                 LogisticRegression(multi_class=&#x27;ovr&#x27;, n_jobs=1, penalty=&#x27;l1&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=80)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;ovr&#x27;, n_jobs=1, penalty=&#x27;l1&#x27;,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=80)),\n",
       "                ('logistic',\n",
       "                 LogisticRegression(multi_class='ovr', n_jobs=1, penalty='l1',\n",
       "                                    solver='liblinear'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_face_recognition.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Logistic Regression\t0.786550\t0.749543\t0.694943\t0.712007\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics for Logistic Regression model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_prediction=pipe_face_recognition.predict(X_test )\n",
    "\n",
    "rec=recall_score(y_test,y_prediction, average='macro')\n",
    "pre=precision_score(y_test,y_prediction, average='macro')\n",
    "acc=accuracy_score(y_test,y_prediction)\n",
    "f1=f1_score(y_test,y_prediction, average='macro')\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Logistic Regression\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(acc,pre,rec,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=30)),\n",
       "                (&#x27;logistic&#x27;,\n",
       "                 LogisticRegression(multi_class=&#x27;ovr&#x27;, n_jobs=1, penalty=&#x27;l1&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=30)),\n",
       "                (&#x27;logistic&#x27;,\n",
       "                 LogisticRegression(multi_class=&#x27;ovr&#x27;, n_jobs=1, penalty=&#x27;l1&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=30)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;ovr&#x27;, n_jobs=1, penalty=&#x27;l1&#x27;,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=30)),\n",
       "                ('logistic',\n",
       "                 LogisticRegression(multi_class='ovr', n_jobs=1, penalty='l1',\n",
       "                                    solver='liblinear'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the number of components to 30 \n",
    "pipe_face_recognition.set_params(pca__n_components=30)\n",
    "\n",
    "\n",
    "pipe_face_recognition.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Logistic Regression\t0.558480\t0.520822\t0.402732\t0.424039\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics for logistic regression with 30 components\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_prediction=pipe_face_recognition.predict(X_test )\n",
    "\n",
    "rec=recall_score(y_test,y_prediction, average='macro')\n",
    "pre=precision_score(y_test,y_prediction, average='macro')\n",
    "acc=accuracy_score(y_test,y_prediction)\n",
    "f1=f1_score(y_test,y_prediction, average='macro')\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Logistic Regression\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(acc,pre,rec,f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using repeated random train split  , 10 runs with 60/40 split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "rs = ShuffleSplit(n_splits=10, test_size=.40)\n",
    "cv_scores = cross_validate(logistic, X_train, y_train, scoring=metrics, cv=rs, n_jobs=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Scores Regression\t0.824375\t0.818204\t0.736914\t0.764106\n"
     ]
    }
   ],
   "source": [
    "recmean=cv_scores['test_recall_macro'].mean() \n",
    "premean=cv_scores['test_precision_macro'].mean() \n",
    "accmean=cv_scores['test_accuracy'].mean()\n",
    "f1mean=cv_scores['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Scores Regression\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmean,premean,recmean,f1mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Std Scores Regression\t0.019605\t0.011786\t0.029646\t0.023834\n"
     ]
    }
   ],
   "source": [
    "recstd=cv_scores['test_recall_macro'].std() \n",
    "prestd=cv_scores['test_precision_macro'].std() \n",
    "accstd=cv_scores['test_accuracy'].std()\n",
    "f1std=cv_scores['test_f1_macro'].std()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Std Scores Regression\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accstd,prestd,recstd,f1std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Scores K-fold\t0.870965\t0.854200\t0.811712\t0.827646\n"
     ]
    }
   ],
   "source": [
    "# Performance using K-fold cross-validation with k=10 \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "cv_scoresK = cross_validate(logistic, X_train, y_train, scoring=metrics, cv=kf, n_jobs=-1)\n",
    "\n",
    "recmeanK=cv_scoresK['test_recall_macro'].mean() \n",
    "premeanK=cv_scoresK['test_precision_macro'].mean() \n",
    "accmeanK=cv_scoresK['test_accuracy'].mean()\n",
    "f1meanK=cv_scoresK['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Scores K-fold\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmeanK,premeanK,recmeanK,f1meanK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|   |Recall   | Precision  |  Accuracy |F-measure   |\n",
    "|---|---|---|---|---|\n",
    "|  Logistic Regression Pipe | 0.6713  | 0.7323  | 0.7573  |0.6946   |\n",
    "|  Shuffle Split|  0.7803  |  0.8058 | 0.8393  |  0.7898 |\n",
    "|  K-fold Cross | 0.80  | 0.81  | 0.84  | 0.79  |\n",
    "\n",
    "\n",
    "\n",
    "The best performance was with the K-fold cross validation which was better in all measures. The fact that k-fold usese repeated sub sampling and that all observations are used for training and validation has improved accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def ml_algorithm_screening_face(X,y,model, model_name, scoring_metrics, pca_dim, n_runs):\n",
    "    estimators = []\n",
    "    seed = 10\n",
    "    if (pca_dim > 0):\n",
    "        estimators.append(('pca', decomposition.PCA(n_components=pca_dim)))\n",
    "    \n",
    "    estimators.append((model_name,model))\n",
    "    pipeline = Pipeline(estimators)\n",
    "    kfold = KFold(n_splits=n_runs, random_state=seed)\n",
    "    \n",
    "    try:\n",
    "        results = cross_val_score(pipeline, X, y, cv=kfold, scoring=scoring_metrics, verbose=1, n_jobs=-1)\n",
    "    except ValueError:\n",
    "        print(\"Opps! something went wrong!\")\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistc regression model using l1 regulaization, pca dimesion = 30, runs =10 \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def ml_custom(X,y,model, scoring_metrics, pca_dim, n_runs):\n",
    "   \n",
    "    \n",
    "    estimators = []\n",
    "    estimators.append(('pca', decomposition.PCA(n_components=pca_dim)))\n",
    "    estimators.steps.append(('logistic', LogisticRegression(penalty='l1')))\n",
    "    model = Pipeline(estimators)\n",
    "    estimators.append(model)\n",
    "    kfold= KFold(n_splits=n_runs)\n",
    "    \n",
    "    try:\n",
    "        results = cross_val_score(model, X, y, cv=kfold, scoring=scoring_metrics, n_jobs=-1)\n",
    "    except ValueError:\n",
    "        print(\"Opps! something went wrong!\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8567251461988304, 0.47368421052631576, 0.5526315789473685, 0.5526315789473685, 0.6111111111111112, 0.6695906432748538, 0.7280701754385965, 0.7719298245614035, 0.7923976608187134, 0.8011695906432749, 0.7982456140350878, 0.8245614035087719, 0.8187134502923976, 0.8304093567251462, 0.847953216374269]\n"
     ]
    }
   ],
   "source": [
    "# Facial recognition has a high-dimensional data problem. We can use PCA to redeuce the number of dimensions. Using a list of dim {10, 20, 30,..., 150}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "\n",
    "pipe_face_recognition = Pipeline(steps=[('pca', pca)])\n",
    "logistic = LogisticRegression(penalty='l1', dual=False,\\\n",
    "              tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,\\\n",
    "              class_weight=None, random_state=None, solver='liblinear', \\\n",
    "            max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "pipe_face_recognition.steps.append(('logistic', logistic))\n",
    "\n",
    "dim_range = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]\n",
    "dimscores = []\n",
    "\n",
    "for d in dim_range:\n",
    "    \n",
    "    pipe_face_recognition.fit(X_train, y_train)\n",
    "    pipe_face_recognition.set_params(pca__n_components=d)\n",
    "    y_prediction=pipe_face_recognition.predict(X_test)\n",
    "    newscores = accuracy_score(y_test,y_prediction)\n",
    "    dimscores.append(newscores)\n",
    "    \n",
    "    \n",
    "\n",
    "print(dimscores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(dimscores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension for PCA that gives the best accuracy is 80 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=80, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing PCA n components to = 80\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pcanew = PCA(n_components=80)\n",
    "pcanew.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Score Logistic\t0.827066\t0.799758\t0.782841\t0.789474\n",
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Scores Discrim\t0.785736\t0.749533\t0.754186\t0.748832\n"
     ]
    }
   ],
   "source": [
    "# Comparing the performance among linear models using 80 dimensions\n",
    "#      Logistc Regression\n",
    "#      Linear Discriminant Analysis \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "logistic = LogisticRegression(n_jobs=-1)\n",
    "metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "cv_scores = cross_validate(logistic, X_train, y_train, scoring=metrics, cv=None, n_jobs=-1)\n",
    "\n",
    "recmeanK=cv_scores['test_recall_macro'].mean() \n",
    "premeanK=cv_scores['test_precision_macro'].mean() \n",
    "accmeanK=cv_scores['test_accuracy'].mean()\n",
    "f1meanK=cv_scores['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Score Logistic\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmeanK,premeanK,recmeanK,f1meanK))\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "cv_scores1 = cross_validate(clf, X_train, y_train, scoring=metrics, cv=None, n_jobs=-1)\n",
    "\n",
    "recmeanC=cv_scores1['test_recall_macro'].mean() \n",
    "premeanC=cv_scores1['test_precision_macro'].mean() \n",
    "accmeanC=cv_scores1['test_accuracy'].mean()\n",
    "f1meanC=cv_scores1['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Scores Discrim\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmeanC,premeanC,recmeanC,f1meanC))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model performs better than the Linear Discriminant Analysis. In all measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Score Support\t0.458651\t0.091730\t0.200000\t0.125774\n",
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Score Classif\t0.510167\t0.291199\t0.320145\t0.287506\n"
     ]
    }
   ],
   "source": [
    "# Comparing the performance among non-linear models using 80 dimensions\n",
    "#      Support Vector Machine (SVM)\n",
    "#      Neural Networks \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clfsvc= SVC()\n",
    "\n",
    "cv_scoressvc = cross_validate(clfsvc, X_train, y_train, scoring=metrics, cv=None, n_jobs=-1)\n",
    "\n",
    "recmeansvc=cv_scoressvc['test_recall_macro'].mean() \n",
    "premeansvc=cv_scoressvc['test_precision_macro'].mean() \n",
    "accmeansvc=cv_scoressvc['test_accuracy'].mean()\n",
    "f1meansvc=cv_scoressvc['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Score Support\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmeansvc,premeansvc,recmeansvc,f1meansvc))\n",
    "\n",
    "clfMLP = MLPClassifier()\n",
    "\n",
    "cv_scoresMLP = cross_validate(clfMLP, X_train, y_train, scoring=metrics, cv=None, n_jobs=-1)\n",
    "\n",
    "recmeanMLP=cv_scoresMLP['test_recall_macro'].mean() \n",
    "premeanMLP=cv_scoresMLP['test_precision_macro'].mean() \n",
    "accmeanMLP=cv_scoresMLP['test_accuracy'].mean()\n",
    "f1meanMLP=cv_scoresMLP['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Score Classif\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmeanMLP,premeanMLP,recmeanMLP,f1meanMLP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP Classifier is more accurate in every measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Score Forest\t0.595206\t0.533858\t0.428514\t0.439805\n",
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Score Kneigh\t0.598998\t0.563443\t0.427152\t0.432219\n",
      "\t\t\tAccuracy\tPrecision\tRecall\t\tF-score\n",
      "Mean Score Guass\t0.520107\t0.475424\t0.489001\t0.465810\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comparing the performance among non-parametric and probabilistic models with 80 dimensions\n",
    "#      Random Forest Classifier\n",
    "#      K-NN Classifer\n",
    "#      GaussianNB\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clfforest = RandomForestClassifier()\n",
    "\n",
    "cv_scoresforest = cross_validate(clfforest, X_train, y_train, scoring=metrics, cv=None, n_jobs=-1)\n",
    "\n",
    "recmeanforest=cv_scoresforest['test_recall_macro'].mean() \n",
    "premeanforest=cv_scoresforest['test_precision_macro'].mean() \n",
    "accmeanforest=cv_scoresforest['test_accuracy'].mean()\n",
    "f1meanforest=cv_scoresforest['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Score Forest\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmeanforest,premeanforest,recmeanforest,f1meanforest))\n",
    "\n",
    "clfKneigh = KNeighborsClassifier()\n",
    "\n",
    "cv_scoresKneigh = cross_validate(clfKneigh, X_train, y_train, scoring=metrics, cv=None, n_jobs=-1)\n",
    "\n",
    "recmeanKneigh=cv_scoresKneigh['test_recall_macro'].mean() \n",
    "premeanKneigh=cv_scoresKneigh['test_precision_macro'].mean() \n",
    "accmeanKneigh=cv_scoresKneigh['test_accuracy'].mean()\n",
    "f1meanKneigh=cv_scoresKneigh['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Score Kneigh\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmeanKneigh,premeanKneigh,recmeanKneigh,f1meanKneigh))\n",
    "\n",
    "\n",
    "clfguass = GaussianNB()\n",
    "\n",
    "cv_scoresguass = cross_validate(clfguass, X_train, y_train, scoring=metrics, cv=None, n_jobs=-1)\n",
    "\n",
    "recmeanguass=cv_scoresguass['test_recall_macro'].mean() \n",
    "premeanguass=cv_scoresguass['test_precision_macro'].mean() \n",
    "accmeanguass=cv_scoresguass['test_accuracy'].mean()\n",
    "f1meanguass=cv_scoresguass['test_f1_macro'].mean()\n",
    "\n",
    "print(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-score\")\n",
    "print(\"Mean Score Guass\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(accmeanguass,premeanguass,recmeanguass,f1meanguass))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNeighbors and Random Forest Classifier are the most accurate. However the Guassian has the best recall and f-score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "|   |Recall   | Precision  |  Accuracy |F-measure   |\n",
    "|---|---|---|---|---|\n",
    "|  Logistic Regression |  0.782841 | 0.799758  |0.827066   | 0.789474  |\n",
    "|  LDA |  0.754186  | 0.749533  | 0.785736 | 0.748832  |\n",
    "|  SVM | 0.200000  |  0.091730 |  0.458651 | 0.125774  |\n",
    "|  Neural Networks |  0.320145 | 0.291199  |  0.510167 |  0.287506 |\n",
    "|  Random Forest |  0.428514 | 0.533858  | 0.595206  | 0.439805  |\n",
    "|  K-NN |  0.427152 | 0.563443  |  0.598998 |  0.432219 |\n",
    "|  GaussianNB | 0.489001  | 0.475424  |  0.520107 |  0.465810 | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistc regression model is the most accurate. It performs the best on every measure. Second is the LDA. The rest of the models all perform quite poorly compared to the LDA and logistc regression model. Particulary the Support Vector Machine. You can see that the linear models perform much better than the non-linear and probablistic models. From this assignment you can conclude that face regonition is a linear problem and it is best to create a linear model to get the best performance. Also learnt was finding the right number of dimensions. Facial recognition databases could contain millions of photos so reducing the dimensions and still keep most of the variance is a good way of improving processing performance. Also learnt is there are a number of ways to measure the performance of a model not just accuracy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
